{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":12285380,"datasetId":7742494,"databundleVersionId":12836224},{"sourceType":"datasetVersion","sourceId":12289320,"datasetId":7745208,"databundleVersionId":12840677}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q \\\n  pandas \\\n  numpy \\\n  scikit-learn \\\n  torch \\\n  transformers \\\n  peft\\\n  accelerate \\\n  bitsandbytes \\\n  nltk \\\n  rouge-score \\\n  tqdm\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:54:38.963789Z","iopub.execute_input":"2025-06-26T14:54:38.964004Z","iopub.status.idle":"2025-06-26T14:56:15.440640Z","shell.execute_reply.started":"2025-06-26T14:54:38.963988Z","shell.execute_reply":"2025-06-26T14:56:15.439856Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Paste your token here\nlogin(\"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\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:56:15.442361Z","iopub.execute_input":"2025-06-26T14:56:15.442613Z","iopub.status.idle":"2025-06-26T14:56:16.095762Z","shell.execute_reply.started":"2025-06-26T14:56:15.442590Z","shell.execute_reply":"2025-06-26T14:56:16.095059Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# fine_tune_llm.py\nimport os\nimport json\nimport torch\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForCausalLM, \n    TrainingArguments, \n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom datasets import Dataset, load_from_disk\nfrom tqdm import tqdm\n\n# ====== CONFIGURATION ======\nDATASET_PATH = \"/kaggle/input/education-dialogue-datasets\"\nCACHE_PATH = \"./cached_education_dataset\"\nMODEL_NAME = \"mistralai/Mistral-7B-v0.1\"\nOUTPUT_DIR = \"./models/llm/fine_tuned_model\"\nDEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\nMAX_LENGTH = 512\nBATCH_SIZE = 2\nEPOCHS = 1  # Start with 1 for speed; increase if time allows\n\nos.makedirs('models/llm', exist_ok=True)\n\ndef load_education_dialogue_dataset(base_path):\n    \"\"\"Load all conversation files from the Education Dialogue Dataset\"\"\"\n    print(\"Loading education dialogue dataset...\")\n    conversation_files = [\n        'conversations_train1.json',\n        'conversations_train2.json',\n        'conversations_train3.json',\n        'conversations_train4.json',\n        'conversations_train5.json',\n        'conversations_eval.json'\n    ]\n    all_data = []\n    for file_name in conversation_files:\n        file_path = os.path.join(base_path, file_name)\n        if os.path.exists(file_path):\n            with open(file_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                all_data.extend(data)\n        else:\n            print(f\"Warning: {file_path} not found\")\n    print(f\"Loaded {len(all_data)} conversations\")\n    return all_data\n\ndef preprocess_and_cache_data(data, tokenizer, max_length=512, cache_path=\"cached_dataset\"):\n    \"\"\"Preprocess data and cache to disk\"\"\"\n    if os.path.exists(cache_path):\n        print(f\"Loading cached dataset from {cache_path}\")\n        return load_from_disk(cache_path)\n    print(\"Preprocessing and caching dataset...\")\n    processed_data = []\n    for item in tqdm(data, desc=\"Processing conversations\"):\n        conv = item.get('conversation', [])\n        background = item.get('background_info', {})\n        if len(conv) < 2:\n            continue\n        for i in range(len(conv) - 1):\n            if conv[i]['role'] == 'Student' and conv[i+1]['role'] == 'Teacher':\n                instruction = (\n                    f\"Background: {json.dumps(background)}\\n\"\n                    f\"Student: {conv[i]['text']}\\n\"\n                    f\"Generate appropriate teacher feedback:\"\n                )\n                response = conv[i+1]['text']\n                text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n                tokenized = tokenizer(\n                    text, \n                    max_length=max_length,\n                    truncation=True,\n                    padding=\"max_length\",\n                    return_tensors=\"pt\"\n                )\n                processed_data.append({\n                    \"input_ids\": tokenized[\"input_ids\"][0],\n                    \"attention_mask\": tokenized[\"attention_mask\"][0]\n                })\n    dataset = Dataset.from_list(processed_data)\n    dataset.save_to_disk(cache_path)\n    return dataset\n\ndef setup_model_and_tokenizer():\n    print(\"Loading Mistral 7B model...\")\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=torch.bfloat16\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map={\"\": 0},  # Force all tensors to GPU 0\n        trust_remote_code=True\n    )\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.pad_token = tokenizer.eos_token\n    model = prepare_model_for_kbit_training(model)\n    lora_config = LoraConfig(\n        r=16,\n        lora_alpha=32,\n        target_modules=[\"q_proj\", \"v_proj\"],\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\"\n    )\n    model = get_peft_model(model, lora_config)\n    model.print_trainable_parameters()\n    return model, tokenizer\n\ndef main():\n    # Setup model and tokenizer\n    model, tokenizer = setup_model_and_tokenizer()\n    # Load and preprocess data\n    raw_data = load_education_dialogue_dataset(DATASET_PATH)\n    train_dataset = preprocess_and_cache_data(\n        raw_data, tokenizer, max_length=MAX_LENGTH, cache_path=CACHE_PATH\n    )\n    # Data collator\n    data_collator = DataCollatorForLanguageModeling(\n        tokenizer=tokenizer,\n        mlm=False\n    )\n    # Training arguments\n    training_args = TrainingArguments(\n        output_dir=OUTPUT_DIR,\n        per_device_train_batch_size=BATCH_SIZE,\n        gradient_accumulation_steps=4,\n        num_train_epochs=EPOCHS,\n        learning_rate=2e-4,\n        fp16=True,\n        save_total_limit=1,\n        logging_steps=50,\n        report_to=\"none\",\n        gradient_checkpointing=True,\n        optim=\"paged_adamw_8bit\",\n        remove_unused_columns=False,\n        max_steps=100 if os.environ.get(\"KAGGLE_KERNEL_RUN_TYPE\", \"\") else None,\n        label_names=[\"labels\"]\n    )\n    # Trainer\n    trainer = Trainer(\n        model=model,\n        train_dataset=train_dataset,\n        data_collator=data_collator,\n        args=training_args\n    )\n    # Fine-tune\n    print(\"Starting fine-tuning...\")\n    trainer.train()\n    # Save the fine-tuned model\n    model.save_pretrained(OUTPUT_DIR)\n    tokenizer.save_pretrained(OUTPUT_DIR)\n    print(\"Fine-tuning completed and model saved!\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-26T14:56:16.096581Z","iopub.execute_input":"2025-06-26T14:56:16.096820Z","iopub.status.idle":"2025-06-26T16:36:34.893937Z","shell.execute_reply.started":"2025-06-26T14:56:16.096777Z","shell.execute_reply":"2025-06-26T16:36:34.893078Z"}},"outputs":[{"name":"stderr","text":"2025-06-26 14:56:35.041258: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750949795.446659      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750949795.558790      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Loading Mistral 7B model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a873cf8c1d045e8b5c82bd90a7ba517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e413131f19d448a880785378b1d71313"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14e5a3c619b4ccf92131105eb37a1cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"493570cfd6784cb284c223429420b01b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d780c96ef37d41e3a75a2088d18e1271"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0fdb95cd8944b36a0a2e89b559507a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b6feb4017a409e8c8b5bdf6fd2688b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ffb913c64c40d6a05137b18d03b55f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31db45cb01dd43809af566336317a6d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45d898d0dbaf42baa99c9da0f0498a77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e1a5abf286d417d95f2312ce3e50739"}},"metadata":{}},{"name":"stdout","text":"trainable params: 6,815,744 || all params: 7,248,547,840 || trainable%: 0.0940\nLoading education dialogue dataset...\nLoaded 47234 conversations\nPreprocessing and caching dataset...\n","output_type":"stream"},{"name":"stderr","text":"Processing conversations: 100%|██████████| 47234/47234 [04:18<00:00, 182.86it/s]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/2 shards):   0%|          | 0/332398 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99c14d2032f44b1bb533fbd876368bed"}},"metadata":{}},{"name":"stdout","text":"Starting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 1:32:09, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>0.376000</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.218600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Fine-tuning completed and model saved!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import shutil\n\n# Define the zip file path and the source directory\nzip_file_path = \"/kaggle/working/working_dir.zip\"\nsource_dir = \"/kaggle/working/\"\n\n# Create the zip file (exclude the zip file itself if it already exists)\nshutil.make_archive(zip_file_path.replace(\".zip\", \"\"), 'zip', source_dir)\n\nprint(\"✅ Done! You can now download 'working_dir.zip' from the Output section.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-26T16:43:05.635860Z","iopub.execute_input":"2025-06-26T16:43:05.636507Z","iopub.status.idle":"2025-06-26T16:43:19.832037Z","shell.execute_reply.started":"2025-06-26T16:43:05.636482Z","shell.execute_reply":"2025-06-26T16:43:19.831239Z"}},"outputs":[{"name":"stdout","text":"✅ Done! You can now download 'working_dir.zip' from the Output section.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}